{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FlKqWptZurZ"
      },
      "source": [
        "# ELT using PySpark on Local with S3\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGwUy2_AZtgD"
      },
      "outputs": [],
      "source": [
        "from pyspark import SparkConf\n",
        "from pyspark.sql import SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FgJ1XRGZ4AY"
      },
      "outputs": [],
      "source": [
        "AWS_ACCESS_KEY_ID = \"YOUR_AWS_ACCESS_KEY_ID\"\n",
        "AWS_SECRET_ACCESS_KEY = \"YOUR_AWS_SECRET_ACCESS_KEY\"\n",
        "AWS_SESSION_TOKEN = \"YOUR_AWS_SESSION_TOKEN\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_s3uHTE5Z5XP"
      },
      "outputs": [],
      "source": [
        "conf = SparkConf()\n",
        "conf.set(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.2.2\")\n",
        "conf.set(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\")\n",
        "conf.set(\"spark.hadoop.fs.s3a.access.key\", AWS_ACCESS_KEY_ID)\n",
        "conf.set(\"spark.hadoop.fs.s3a.secret.key\", AWS_SECRET_ACCESS_KEY)\n",
        "conf.set(\"spark.hadoop.fs.s3a.session.token\", AWS_SESSION_TOKEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bp1_1w2bZ5eH"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfmO8_c1aCip"
      },
      "outputs": [],
      "source": [
        "df = spark.read.csv(\"s3://tands525/price_paid_records01.csv\", header=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iP35KYsEaEYB"
      },
      "outputs": [],
      "source": [
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2xbp_o3aFKx"
      },
      "outputs": [],
      "source": [
        "df.createOrReplaceTempView(\"housingprice\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "McAuqQEmaGLx"
      },
      "outputs": [],
      "source": [
        "table = spark.sql(\"\"\"\n",
        "    select\n",
        "        Transaction unique identifier\n",
        "        , Price\n",
        "        , Date of Transfer \n",
        "        , Property Type\n",
        "        , Old/New\n",
        "        , Duration\n",
        "        , Town/City\n",
        "        , District\n",
        "        , County\n",
        "        , PPDCategory Type\n",
        "        , Record Status - monthly file only\n",
        "        \n",
        "    from\n",
        "        housingprice\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnNa58hQaIiK"
      },
      "outputs": [],
      "source": [
        "table.write.mode(\"overwrite\").csv(\"s3://tands525/price_paid_records01.csv\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
